{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression  Assignment"
      ],
      "metadata": {
        "id": "2CFaahzC9BK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear\n",
        "Regression?\n",
        "\n",
        "Ans:Logistic regression and linear regression are both supervised learning algorithms, but they differ in their purpose and how they handle the output variable. Linear regression predicts continuous numerical values, while logistic regression predicts the probability of a categorical outcome. In simpler terms, linear regression is used for things like predicting house prices, while logistic regression is used for things like predicting whether an email is spam or not.\n",
        "\n",
        "Here's a more detailed breakdown:\n",
        "\n",
        "1. Linear Regression:\n",
        "\n",
        " - Objective:\n",
        "Predicts a continuous dependent variable (output) based on one or more independent variables (inputs).\n",
        "\n",
        " - Output:\n",
        "A numerical value that can fall anywhere on a continuous scale (e.g., temperature, sales figures, house price).\n",
        "\n",
        " - Example:\n",
        "Predicting the price of a house based on its size, location, and number of bedrooms.\n",
        "\n",
        "  - Assumptions:\n",
        "\n",
        "- Linear relationship between dependent and independent variables.\n",
        "\n",
        "- Residuals (errors) are normally distributed.\n",
        "\n",
        "- Homoscedasticity (constant variance of errors).\n",
        "\n",
        "- Estimation Method: Ordinary Least Squares (OLS), minimizing the sum of squared differences between observed and predicted values.\n",
        "\n",
        "- Evaluation Metrics: R-squared, Mean Squared Error (MSE), Root Mean Squared Error (RMSE).\n",
        "\n",
        "2. Logistic Regression:\n",
        "\n",
        " - Objective:\n",
        "Predicts the probability of a categorical dependent variable belonging to a specific class.\n",
        "\n",
        " - Output:\n",
        "A probability value between 0 and 1, representing the likelihood of the event occurring (e.g., probability of a customer clicking on an ad, probability of a patient having a disease).\n",
        "\n",
        " - Example:\n",
        "Predicting whether a customer will click on an advertisement (yes or no), or classifying an email as spam or not spam.\n",
        "\n",
        " - Assumptions:\n",
        "\n",
        "- The dependent variable is binary.\n",
        "\n",
        "- Observations are independent.\n",
        "\n",
        "- Little or no multicollinearity among independent variables.\n",
        "\n",
        "- Estimation Method: Maximum Likelihood Estimation (MLE), finding parameters that maximize the likelihood of observing the given data.\n",
        "\n",
        "- Evaluation Metrics: Accuracy, Precision, Recall, F1 Score, Area Under the ROC Curve (AUC-ROC)\n",
        "\n",
        " - Key feature:\n",
        "Uses a sigmoid function (or similar) to transform the linear combination of input variables into a probability.\n",
        "\n",
        "Practical Applications\n",
        "\n",
        " 1.Linear Regression:\n",
        "\n",
        " - Forecasting sales revenue based on advertising spend.\n",
        "\n",
        " - Estimating a person's weight based on height and age.\n",
        "\n",
        "2. Logistic Regression:\n",
        "\n",
        "- Determining whether a customer will buy a product (yes/no).\n",
        "\n",
        "- Predicting if an email is spam or not\n"
      ],
      "metadata": {
        "id": "PnhKRz7a9IBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain the role of the Sigmoid function in Logistic Regression.?\n",
        "\n",
        "Ans:In Logistic Regression, the sigmoid function, also known as the logistic function, plays a crucial role in transforming the output of a linear model into a probability value between 0 and 1. This transformation allows for binary classification, where the model predicts the probability of an instance belonging to a particular class.\n",
        "\n",
        "Here's a more detailed explanation:\n",
        "1. Linear Model Output:\n",
        "Logistic Regression starts with a linear model that combines input features with weights and a bias term. This linear equation produces a real-valued output (z).\n",
        "\n",
        "2. Sigmoid Transformation:\n",
        "The sigmoid function (œÉ(z)) then takes this real-valued output (z) and maps it to a value between 0 and 1. The sigmoid function is defined as: œÉ(z) = 1 / (1 + exp(-z)).\n",
        "\n",
        "3. Probability Interpretation:\n",
        "The output of the sigmoid function (which is between 0 and 1) is interpreted as the probability of the instance belonging to the \"positive\" class (e.g., the probability of a customer clicking on an ad, or the probability of a medical diagnosis being malignant).\n",
        "\n",
        "4. Binary Classification:\n",
        "Based on a threshold (usually 0.5), the predicted probability is used to classify the instance. If the probability is above the threshold, the instance is classified as belonging to the positive class; otherwise, it's classified as belonging to the negative class.\n",
        "\n",
        "The sigmoid function allows logistic regression to model probabilities, making it suitable for binary classification tasks. It converts the unbounded output of the linear model into a meaningful probability, which is then used to make class predictions.\n",
        "\n",
        "\n",
        "How It‚Äôs Used in Logistic Regression:\n",
        "\n",
        " - Mapping linear scores to probabilities\n",
        "The model first computes a linear combination of inputs:\n",
        "\n",
        "ùëß\n",
        "=\n",
        "ùõΩ\n",
        "0\n",
        "+\n",
        "ùõΩ\n",
        "1\n",
        "ùë•\n",
        "1\n",
        "+\n",
        "‚ãØ\n",
        "+\n",
        "ùõΩ\n",
        "ùëò\n",
        "ùë•\n",
        "ùëò\n",
        "z=Œ≤\n",
        "0\n",
        "‚Äã\n",
        " +Œ≤\n",
        "1\n",
        "‚Äã\n",
        " x\n",
        "1\n",
        "‚Äã\n",
        " +‚ãØ+Œ≤\n",
        "k\n",
        "‚Äã\n",
        " x\n",
        "k\n",
        "‚Äã\n",
        "\n",
        " - Then applies the sigmoid to turn\n",
        "ùëß\n",
        "z (which can be any real number) into a probability\n",
        "ùëù\n",
        "=\n",
        "ùúé\n",
        "(\n",
        "ùëß\n",
        ")\n",
        "p=œÉ(z) ‚Äî i.e., the estimated likelihood that the outcome is ‚Äú1‚Äù\n",
        "\n",
        "\n",
        " - Interpreting probabilities as classes\n",
        "These probability outputs can be:\n",
        "\n",
        "Used directly (say, giving a 93.2% chance of being in class 1)\n",
        "\n",
        "Thresholded (commonly at 0.5) to assign a class label:\n",
        "ùëù\n",
        "‚â•\n",
        "0.5\n",
        "‚Üí\n",
        "class¬†1\n",
        ",\n",
        "p‚â•0.5‚Üíclass¬†1, otherwise class 0\n",
        "\n",
        " - Enabling probability-based modeling\n",
        "This mapping lets logistic regression interpret outputs probabilistically, unlike methods like linear discriminant analysis or SVMs\n",
        "Data Science Stack Exchange\n",
        "\n",
        "\n",
        "Why the Sigmoid Function is Ideal\n",
        "\n",
        " - Bounded and continuous: Always outputs a smooth probability between 0 and 1 ‚Äî vital for modeling real-world outcomes\n",
        "\n",
        "\n",
        " - Differentiable: Facilitates gradient-based optimization (e.g., maximizing the likelihood) for parameter estimation.\n",
        "\n",
        "\n",
        " - Statistical justification: It naturally emerges when modeling the log-odds (logit)‚Äîthat is, logistic regression can be framed as modeling a linear function of the log-odds of the outcome, which upon inversion yields the sigmoid"
      ],
      "metadata": {
        "id": "9pF6czic-w_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  What is Regularization in Logistic Regression and why is it needed?\n",
        "\n",
        "Ans:Regularization in logistic regression is a technique used to prevent overfitting by adding a penalty term to the loss function. This penalty discourages overly complex models, leading to better generalization performance on unseen data. Essentially, it trades a slight increase in training error for a larger decrease in generalization error.\n",
        "\n",
        "Why is it needed\n",
        "\n",
        " - Overfitting:\n",
        "Without regularization, logistic regression models can become too complex and overfit the training data, meaning they perform well on the training set but poorly on new, unseen data.\n",
        "\n",
        " - Generalization:\n",
        "Regularization helps logistic regression models generalize better to new data by simplifying the model and preventing it from learning noise or irrelevant details in the training data.\n",
        "\n",
        " - Feature Selection:\n",
        "Regularization can also be used for feature selection by shrinking the coefficients of less important features towards zero, effectively removing them from the model.\n",
        "\n",
        "\n",
        "\n",
        " - Balances bias‚Äìvariance tradeoff: Regularization adds slight bias while reducing variance, helping you avoid both overfitting and underfitting if tuned properly.\n",
        "\n",
        " - Handles multicollinearity and noisy data: It distributes the weight more sensibly among correlated features, improving robustness in the presence of redundant or noisy predictors.\n",
        "\n",
        " - Enhances interpretability: Especially with L1 regularization, the model becomes sparser and easier to understand and explain.\n",
        "\n",
        "How it works:\n",
        "\n",
        " - Regularization adds a penalty term to the loss function, which is the function that the model tries to minimize during training.\n",
        "\n",
        " - This penalty term is typically based on the magnitude of the model's coefficients (weights).\n",
        "Common types of regularization include L1 (Lasso) and L2 (Ridge) regularization, which penalize the sum of absolute values and the sum of squared values of the coefficients, respectively.\n",
        "\n",
        " - By adding this penalty, regularization encourages the model to find simpler solutions with smaller coefficient values, leading to better generalization.\n",
        "\n",
        " - Regularization helps logistic regression models to strike a balance between fitting the training data well and avoiding overfitting, resulting in more reliable and accurate predictions on new data.\n",
        "\n",
        "\n",
        "\n",
        "What Regularization Does in Logistic Regression\n",
        "\n",
        " - Penalizes large coefficients: Regularization adds a term to the loss function that depends on the magnitude of the model‚Äôs weights, thereby reducing the risk of the model fitting to random noise.\n",
        "\n",
        " - Controls model complexity: By imposing a constraint (‚Äúbudget‚Äù) on how large weights can grow, regularization prevents the model from becoming too flexible and overfitting.\n",
        "\n",
        " - Improves stability and convergence: Especially in high-dimensional or sparse settings, regularization can help logistic regression converge more reliably and avoid unstable solutions\n",
        "\n",
        "\n",
        " Regularization is a powerful tool in logistic regression to:\n",
        "\n",
        "1. Avoid overfitting\n",
        "\n",
        "2. Improve model generalization\n",
        "\n",
        "3. Stabilize learning in high-dimensional or noisy settings\n",
        "\n",
        "4. Enable feature selection and interpretability (especially via L1)\n",
        "\n",
        "5. The choice between L1, L2, or Elastic Net‚Äîand the tuning of the regularization strength (often via Œª or inverse parameter\n",
        "ùê∂\n",
        "C in libraries like scikit-learn)‚Äîshould be guided by your data characteristics and modeling goals."
      ],
      "metadata": {
        "id": "oebA_DtZ_5YI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are some common evaluation metrics for classification models, and\n",
        "why are they important?\n",
        "\n",
        "\n",
        "Ans:Common evaluation metrics for classification models include accuracy, precision, recall, F1-score, and AUC-ROC. These metrics help assess how well a model distinguishes between different classes, particularly in cases of imbalanced datasets, and guide model selection and improvement.\n",
        "\n",
        "Here's a breakdown of these metrics:\n",
        "\n",
        "1. Accuracy:\n",
        "\n",
        " - Definition:\n",
        "The proportion of correctly classified instances out of the total number of instances.\n",
        "\n",
        " - Formula:\n",
        "(True Positives + True Negatives) / (True Positives + True Negatives + False Positives + False Negatives)\n",
        "\n",
        " - Importance:\n",
        "While simple and widely used, accuracy can be misleading for imbalanced datasets, where one class has significantly more instances than the other.\n",
        "\n",
        "2. Precision:\n",
        "\n",
        " - Definition: The proportion of correctly predicted positive instances out of all instances predicted as positive.\n",
        "\n",
        " - Formula: True Positives / (True Positives + False Positives)\n",
        "\n",
        " - Importance: Useful when the cost of a false positive is high. For example, in spam detection, we want to minimize the number of legitimate emails wrongly classified as spam.\n",
        "\n",
        "3. Recall (also known as Sensitivity or True Positive Rate):\n",
        "\n",
        " - Definition: The proportion of correctly predicted positive instances out of all actual positive instances.\n",
        "\n",
        " - Formula: True Positives / (True Positives + False Negatives)\n",
        "\n",
        " - Importance: Useful when the cost of a false negative is high. For example, in medical diagnosis, we want to minimize the number of infected patients wrongly classified as not infected.\n",
        "\n",
        "4. F1-score:\n",
        "\n",
        " - Definition: The harmonic mean of precision and recall, providing a balanced measure of both.\n",
        "\n",
        " - Formula: 2 * (Precision * Recall) / (Precision + Recall)\n",
        "\n",
        " - Importance: Useful when you need a balance between precision and recall, especially in situations where both false positives and false negatives are undesirable.\n",
        "\n",
        "5. AUC-ROC (Area Under the Receiver Operating Characteristic Curve):\n",
        "\n",
        " - Definition:\n",
        "Measures the model's ability to distinguish between classes across various probability thresholds.\n",
        "\n",
        " - Importance:\n",
        "Effective for evaluating binary classification models, especially when dealing with imbalanced datasets. A higher AUC-ROC indicates better performance.\n",
        "\n",
        "Why are these metrics important\n",
        "\n",
        " - Model Evaluation:\n",
        "They provide a quantitative way to assess the performance of a classification model, helping to identify strengths and weaknesses.\n",
        "\n",
        " - Model Selection:\n",
        "By comparing different models based on these metrics, you can choose the one that best suits the specific problem and data characteristics.\n",
        "\n",
        " - Parameter Tuning:\n",
        "Metrics guide the optimization process, helping to fine-tune model parameters for improved performance.\n",
        "\n",
        " - Imbalanced Datasets:\n",
        "They are crucial for evaluating models on datasets where the classes are not equally represented.\n",
        "\n",
        "\n",
        "\n",
        "More Advanced Metrics\n",
        "\n",
        " - Log Loss / Cross-Entropy\n",
        "Measures how well the predicted probabilities align with actual outcomes. A lower score indicates better-calibrated predictions. Particularly useful when probability estimates matter.\n",
        "\n",
        "\n",
        " - Cohen‚Äôs Kappa\n",
        "Accounts for agreement occurring by chance, offering a more nuanced evaluation, especially for imbalanced datasets.\n",
        "\n",
        "\n",
        " - Matthews Correlation Coefficient (MCC)\n",
        "Incorporates all elements of the confusion matrix and is generally regarded as one of the most informative single-value metrics, especially in the presence of imbalanced classes.\n",
        "\n",
        "\n",
        " - Other metrics:\n",
        "Balanced accuracy, F-beta, expected cost, Brier score, and calibration loss also offer value depending on the specific application and output type.\n",
        "\n",
        "\n",
        "selecting the appropriate evaluation metrics and understanding their implications is vital for building effective and reliable classification models."
      ],
      "metadata": {
        "id": "EdhoTEYeBi92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 5. Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
        "# splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
        "#(Use Dataset from sklearn package) ?\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification # For creating a sample dataset\n",
        "\n",
        "# 1. Load the dataset into a Pandas DataFrame\n",
        "#    Replace this with pd.read_csv('your_file.csv') for your own data\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
        "df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
        "df['target'] = y\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "#    test_size: proportion of the dataset to include in the test split\n",
        "#    random_state: ensures reproducibility of the split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train a Logistic Regression model\n",
        "#    random_state: ensures reproducibility of the model training\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 5. Print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Logistic Regression model: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di7HVTENFS8G",
        "outputId": "bee3c22a-9dee-4ade-c7f1-80c5bb4a9b26"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Logistic Regression model: 0.8300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Write a Python program to train a Logistic Regression model using L2\n",
        "#regularization (Ridge) and print the model coefficients and accuracy.\n",
        "#(Use Dataset from sklearn package).\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a sample dataset (Breast Cancer dataset)\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model with L2 regularization\n",
        "# The 'penalty' parameter is set to 'l2' for Ridge regularization.\n",
        "# The 'C' parameter is the inverse of regularization strength; smaller values mean stronger regularization.\n",
        "model = LogisticRegression(penalty='l2', C=1.0, solver='liblinear', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print the model coefficients\n",
        "print(\"Model Coefficients:\")\n",
        "for i, coef in enumerate(model.coef_[0]):\n",
        "    print(f\"Feature {i}: {coef:.4f}\")\n",
        "print(f\"Intercept: {model.intercept_[0]:.4f}\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCGU4ytRF8X1",
        "outputId": "88b24540-9a59-4fc5-8d01-4c2d7a7a09e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients:\n",
            "Feature 0: 2.1325\n",
            "Feature 1: 0.1528\n",
            "Feature 2: -0.1451\n",
            "Feature 3: -0.0008\n",
            "Feature 4: -0.1426\n",
            "Feature 5: -0.4156\n",
            "Feature 6: -0.6519\n",
            "Feature 7: -0.3445\n",
            "Feature 8: -0.2076\n",
            "Feature 9: -0.0298\n",
            "Feature 10: -0.0500\n",
            "Feature 11: 1.4430\n",
            "Feature 12: -0.3039\n",
            "Feature 13: -0.0726\n",
            "Feature 14: -0.0162\n",
            "Feature 15: -0.0019\n",
            "Feature 16: -0.0449\n",
            "Feature 17: -0.0377\n",
            "Feature 18: -0.0418\n",
            "Feature 19: 0.0056\n",
            "Feature 20: 1.2321\n",
            "Feature 21: -0.4046\n",
            "Feature 22: -0.0362\n",
            "Feature 23: -0.0271\n",
            "Feature 24: -0.2626\n",
            "Feature 25: -1.2090\n",
            "Feature 26: -1.6180\n",
            "Feature 27: -0.6153\n",
            "Feature 28: -0.7428\n",
            "Feature 29: -0.1170\n",
            "Intercept: 0.4085\n",
            "\n",
            "Model Accuracy: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Write a Python program to train a Logistic Regression model for multiclass\n",
        "#classification using multi_class='ovr' and print the classification report.\n",
        "#(Use Dataset from sklearn package)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Load a multiclass dataset ‚Äî e.g., Iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split into train and test sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Instantiate and train logistic regression with One-vs-Rest strategy\n",
        "model = LogisticRegression(\n",
        "    multi_class='ovr',  # One-vs-Rest approach\n",
        "    solver='liblinear',  # Supports 'ovr' for multiclass\n",
        "    max_iter=1000\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 5. Print classification report (precision, recall, F1-score per class)\n",
        "print(\"Classification Report:\\n\",\n",
        "      classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMpJyhGvIpAG",
        "outputId": "504891a9-e16f-4e46-c1d2-b0e5be96daf7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      1.00      1.00         9\n",
            "   virginica       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.  Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "#hyperparameters for Logistic Regression and print the best parameters and validation\n",
        "#accuracy.\n",
        "#(Use Dataset from sklearn package)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a dataset from sklearn (e.g., Iris dataset)\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "logistic_regression = LogisticRegression(max_iter=1000, solver='liblinear') # 'liblinear' supports both 'l1' and 'l2' penalties\n",
        "\n",
        "# Define the parameter grid for C and penalty\n",
        "param_grid = {\n",
        "    'C': np.logspace(-4, 4, 9),  # Example C values from 10^-4 to 10^4\n",
        "    'penalty': ['l1', 'l2']     # L1 and L2 regularization\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "# cv=5 for 5-fold cross-validation\n",
        "# scoring='accuracy' to evaluate performance based on accuracy\n",
        "grid_search = GridSearchCV(logistic_regression, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by GridSearchCV\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Print the best validation accuracy achieved during the grid search\n",
        "print(f\"Best validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Optionally, evaluate the best model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test set accuracy with best model: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDf9ZZ7WJM4f",
        "outputId": "d8479629-2e97-44df-e1b3-64eee7e213c6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "Best parameters: {'C': np.float64(1000.0), 'penalty': 'l2'}\n",
            "Best validation accuracy: 0.9667\n",
            "Test set accuracy with best model: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  9. Write a Python program to standardize the features before training Logistic\n",
        "#Regression and compare the model's accuracy with and without scaling.\n",
        "#(Use Dataset from sklearn package)\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a dataset (e.g., Breast Cancer dataset)\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- Model without scaling ---\n",
        "print(\"--- Model without scaling ---\")\n",
        "logistic_regression_unscaled = LogisticRegression(max_iter=1000, random_state=42)\n",
        "logistic_regression_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = logistic_regression_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "print(f\"Accuracy without scaling: {accuracy_unscaled:.4f}\")\n",
        "\n",
        "# --- Model with scaling ---\n",
        "print(\"\\n--- Model with scaling (StandardScaler) ---\")\n",
        "\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform both training and testing data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression on scaled data\n",
        "logistic_regression_scaled = LogisticRegression(max_iter=1000, random_state=42)\n",
        "logistic_regression_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = logistic_regression_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Accuracy with scaling: {accuracy_scaled:.4f}\")\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"\\nAccuracy difference (scaled - unscaled): {accuracy_scaled - accuracy_unscaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrraAJThKAAL",
        "outputId": "c71cc423-be38-4a0a-8ea5-cba264272816"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model without scaling ---\n",
            "Accuracy without scaling: 0.9708\n",
            "\n",
            "--- Model with scaling (StandardScaler) ---\n",
            "Accuracy with scaling: 0.9825\n",
            "\n",
            "Accuracy difference (scaled - unscaled): 0.0117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced\n",
        "dataset (only 5% of customers respond), describe the approach you‚Äôd take to build a\n",
        "Logistic Regression model ‚Äî including data handling, feature scaling, balancing\n",
        "classes, hyperparameter tuning, and evaluating the model for this real-world business\n",
        "use case.?\n",
        "\n",
        "Ans:Building a Logistic Regression model for predicting marketing campaign responders with an imbalanced dataset requires a systematic approach.\n",
        "\n",
        " 1. Data Handling:\n",
        "\n",
        " - Exploratory Data Analysis (EDA):\n",
        "Understand the features, identify potential outliers, and assess missing values.\n",
        "\n",
        " - Feature Engineering:\n",
        "Create new features from existing ones that might be more predictive (e.g., customer lifetime value, recency of last purchase, frequency of purchases).\n",
        "\n",
        " - Handling Missing Values:\n",
        "Impute missing values using appropriate strategies like mean, median, mode imputation, or more advanced methods like K-Nearest Neighbors (KNN) imputation.\n",
        "\n",
        " 2. Feature Scaling:\n",
        "Logistic Regression is sensitive to feature scales. Apply Standardization (Z-score normalization) or Min-Max Scaling to ensure all features contribute equally to the model.\n",
        "\n",
        " - Python\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        " 3. Balancing Classes:\n",
        "Given the 5% response rate, addressing class imbalance is crucial.\n",
        "\n",
        " - Oversampling the minority class: Techniques like SMOTE (Synthetic Minority Over-sampling Technique) generate synthetic samples of the minority class, increasing its representation.\n",
        "\n",
        " - Undersampling the majority class: Randomly remove samples from the majority class to balance the dataset.\n",
        "\n",
        " - Class Weights: Assign higher weights to the minority class during model training. This tells the model to pay more attention to correctly classifying the minority class.\n",
        "\n",
        " - Python\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
        "\n",
        " 4. Hyperparameter Tuning:\n",
        "Use techniques like Grid Search or Randomized Search with cross-validation to find the optimal hyperparameters for the Logistic Regression model.\n",
        "Key hyperparameters to tune include C (regularization strength) and solver.\n",
        "\n",
        " - Python\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'solver': ['liblinear', 'lbfgs']}\n",
        "grid_search = GridSearchCV(LogisticRegression(random_state=42), param_grid, cv=5, scoring='f1')\n",
        "grid_search.fit(X_resampled, y_resampled)\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        " 5. Model Evaluation:\n",
        "\n",
        " - Metrics for Imbalanced Data:\n",
        "Precision, Recall, and F1-score: Focus on these for the minority class (responders) as accuracy can be misleading.\n",
        "\n",
        " - ROC AUC (Receiver Operating Characteristic Area Under the Curve): Provides a comprehensive measure of the model's ability to distinguish between classes.\n",
        "\n",
        " -Precision-Recall Curve: Useful for visualizing the trade-off between precision and recall at different thresholds.\n",
        "\n",
        " - Cross-validation:\n",
        "Use techniques like K-fold cross-validation to get a robust estimate of model performance and prevent overfitting.\n",
        "\n",
        " - Business Context:\n",
        "Evaluate the model's performance in terms of business impact, considering the cost of false positives (sending campaigns to non-responders) and false negatives (missing potential responders). A high recall for responders might be prioritized even if it means a slightly lower precision."
      ],
      "metadata": {
        "id": "iQS66y1RLJId"
      }
    }
  ]
}